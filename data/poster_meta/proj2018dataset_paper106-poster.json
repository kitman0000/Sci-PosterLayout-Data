{"poster_size": 4478976, "doclen": 249, "figlen": 739412, "section": {"1": {"xy": [281, 37, 2155, 100], "panel_size": 215500, "panel_aspectRatio": 21.55, "category": "Title", "figures": 0, "textlen": 0, "fig_size": 0, "title": "Deep Cue Learning: A Reinforcement Learning Agent for Pool", "textRatio": 0.0, "figRatio": 0.0}, "2": {"xy": [21, 272, 867, 376], "panel_size": 325992, "panel_aspectRatio": 2.3058510638297873, "category": "Introduction", "figures": 0, "textlen": 49, "fig_size": 0, "title": "Summary", "textRatio": 0.19678714859437751, "figRatio": 0.0}, "4": {"xy": [23, 669, 860, 360], "panel_size": 309600, "panel_aspectRatio": 2.388888888888889, "category": "Introduction", "figures": 0, "textlen": 0, "fig_size": 0, "title": "The goal of this project is to apply ReinforcementLearning to the game of pool.\nThe environment is formulated as an MDP and solvedwith Q-Table, DQN, and A3C algorithms.\nWith two balls on the table, Q-Table learns the best, butA3C with discrete action space has the best performanceonsidering all trade-offs.", "textRatio": 0.0, "figRatio": 0.0}, "5": {"xy": [21, 1047, 855, 681], "panel_size": 582255, "panel_aspectRatio": 1.2555066079295154, "category": "Method", "figures": 1, "textlen": 67, "fig_size": 43460, "title": "Algorithms", "9": {"fig_size": 43460, "fig_aspectRatio": 1.0341463414634147, "fig_size_poster": 0.247953216374269}, "textRatio": 0.26907630522088355, "figRatio": 0.05877643316581283}, "10": {"xy": [906, 267, 1645, 585], "panel_size": 962325, "panel_aspectRatio": 2.8119658119658117, "category": "Result", "figures": 3, "textlen": 0, "fig_size": 695952, "title": "Experimental Results", "11": {"fig_size": 313120, "fig_aspectRatio": 3.388157894736842, "fig_size_poster": 0.6261398176291794}, "12": {"fig_size": 220800, "fig_aspectRatio": 11.594202898550725, "fig_size_poster": 0.9726443768996961}, "13": {"fig_size": 162032, "fig_aspectRatio": 1.7532894736842106, "fig_size_poster": 0.32401215805471123}, "textRatio": 0.0, "figRatio": 0.9412235668341872}, "14": {"xy": [906, 884, 865, 815], "panel_size": 704975, "panel_aspectRatio": 1.0613496932515338, "category": "Discussion", "figures": 0, "textlen": 111, "fig_size": 0, "title": "Discussion", "textRatio": 0.4457831325301205, "figRatio": 0.0}, "16": {"xy": [1783, 954, 763, 273], "panel_size": 208299, "panel_aspectRatio": 2.7948717948717947, "category": "Discussion", "figures": 0, "textlen": 0, "fig_size": 0, "title": "Four-Ball Environment\nBoth A3C with continuous and discrete action performpoorly when state space is enlarged.In continuous action, values tend to be saturated andclipped at 0 or 1; in discrete action, a single anglevalue tends to be favored.", "textRatio": 0.0, "figRatio": 0.0}, "17": {"xy": [1786, 1252, 785, 290], "panel_size": 227650, "panel_aspectRatio": 2.706896551724138, "category": "Discussion", "figures": 0, "textlen": 22, "fig_size": 0, "title": "Future Work", "textRatio": 0.08835341365461848, "figRatio": 0.0}}}